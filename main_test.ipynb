{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is the test part of the final model. Running it will start a pygame window using the learnt policy in gaming.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project in brief\n",
    "- **In this project, we try to train a ping pong player who learns the direction and distance of optimal movement through:**\n",
    "    + A six-layered Convolutional Neural Network to recognize ball behavior from updated screen input\n",
    "    + A reinforcement learning system which targets the learning agent at and against behaviors e.g. Catching the ball, avoiding loops\n",
    "- **Game rules:** \n",
    "    + The ball is served by the opponent AI first, at randomd angle; Upon wining, the serving right is passed to the opponent\n",
    "    + Score in testing (shown on pygame screen) is calculated as one additional points if opponent fails to catch the ball\n",
    "- **File structure** - Project.zip contains:\n",
    "    + A *pygame_player.py* for overall functions\n",
    "    + A *pong.py* for defining opponent AI and screen display configurations\n",
    "    + A *pong_player.py* for defining example class for playing pong which imports from pygame_player.py\n",
    "    + A *main_train.ipynb* for training the learning agent\n",
    "    + A *main_test.ipynb* for testing and demonstrating agent performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our design\n",
    "- **Opponent player** - We designed the opponent pong player to be competent while vulnerable: \n",
    "    + Competent: In order for it to catch most balls, we make the opponent cheat by\n",
    "        - reading the ball's direction from coordinates\n",
    "        - aligning the ball's speed with its own speed\n",
    "    + Vulnerable: \n",
    "        - The opponent's action is defined at a random rate\n",
    "        - Due to limited speed, it cannot catch certain servings e.g. when the ball heads from upper half towards upper corner while it is in the lower half of the field\n",
    "- **RL process:**\n",
    "    + Observation VS exploring - We settled on 100k observations steps and 1000k exploring steps. To balance the exploration/exploitation tradeoff, we raised the observation steps after finding our agent trapped in a local optimal policy to only move downwards\n",
    "    + Learning reward - We designed the reward system to enhance several actions:\n",
    "        - For the first catch in a round, whichever player catches the ball is rewarded 3 points. We encourage our agent to catch more serve with this design\n",
    "        - For continuing catch in the round, reward is given at $3*0.6^{catch times}$. We discourage our agent to be trapped in tacit agreement to stay put and make every catch\n",
    "- **CNN network:**\n",
    "    + The structure is based on DeepMind\n",
    "    + We improved it with ReLu, adding one maxpooling layer for stability, and adding one dropout layer against overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement \n",
    "0. **Model 0 Baseline: **\n",
    "    - Features: \n",
    "        + Basic reward (1 point if opponent fail to catch)\n",
    "        + 2-directioned serve (ball starts from the middle line 45 degrees to one of the four corners)\n",
    "        + Basic Network(6-layered)\n",
    "        + 20k observation steps\n",
    "    - Performace (at 500k steps): [Video performance model baseline](https://drive.google.com/open?id=1OvXVqnOP3i5ZNfkGtW7FN4bTQcV-PSYY)\n",
    "    - Problems: \n",
    "        + Bad performance on one of two directions\n",
    "        + Too simple rules - will underperform in other cases\n",
    "    - Code: [GitHub model baseline](https://github.com/KaiChenColumbia/MagicBalls/tree/master/model_base)\n",
    "\n",
    "1. **Model 1: **\n",
    "    - Improvement: In terms of the above problems, we - \n",
    "        + To raise the difficulty of the game and allowing more complex cases, we changed the serving to random directions, in addition to features of the baseline model.\n",
    "    - Performace (at 700k steps): [Video performance model 1](https://drive.google.com/open?id=1lUDO4R8-L9MkmMayr5M6QSiiJW7KhvAi)\n",
    "    - Problems: \n",
    "        + Local optimal - the bar was easily trapped in a local policy of only moving downwards/upwards\n",
    "        + Not amazing performace - positive but slow linear learning rate \n",
    "    - Code: [GitHub model 1](https://github.com/KaiChenColumbia/MagicBalls/tree/master/model1)\n",
    "    \n",
    "2. **Model 2: **\n",
    "    - Improvement: In terms of the above problems, we -\n",
    "        + increased the number of observation steps to 50000, and created checkpoints every 50000 steps to check how much performance has improved\n",
    "        + raised the difficulty of the game by serving the ball at random angle\n",
    "        + improved CNN to adopt DeepMind structure\n",
    "    - Performance (at 600k steps): [Video performance model 2](https://drive.google.com/open?id=1exXiiL2s7iMSdHrc8GToFHk9dTFoaUx_)\n",
    "    - Problems: New problems emerged - \n",
    "        + The two bars reached a “consensus” to stay in diagonal corners with which minimal movement ensures no one loses the game, this resulted in no update of the policy\n",
    "        + Still learning rate (We considered it to be the result of harder rules/stronger opponent thus fewer positive reward)\n",
    "    - Code: [GitHub model 2](https://github.com/KaiChenColumbia/MagicBalls/tree/master/model2)\n",
    "    \n",
    "3. **Model 3 Final version:**\n",
    "    - Improvement: In terms of the above problems, we -\n",
    "        + updated reward system to penalize repeated trajectory of the ball movement. Namely, reward will experience an exponential decay with a rate of 0.6 every time it hits the bar in a specific round, and the loop is forced to break after 10 hits\n",
    "        + upgraded CNN with RELU\n",
    "    - Performance (at 1000k steps): [Video performance model 3](https://drive.google.com/open?id=1othJyEePWy1O4lo9Et4UJgV19X6oJd4Z)\n",
    "    - Code: [GitHub model 3](https://github.com/KaiChenColumbia/MagicBalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import random\n",
    "from collections import deque\n",
    "from pong_player import PongPlayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pygame.constants import K_DOWN, K_UP, K_RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQPongPlayerTest(PongPlayer):\n",
    "    ACTIONS_COUNT = 3  # number of valid actions. In this case up, still and down\n",
    "    # FUTURE_REWARD_DISCOUNT = 0.99  # decay rate of past observations\n",
    "    OBSERVATION_STEPS = 20000.  # time steps to observe before training\n",
    "    EXPLORE_STEPS = 500000.  # frames over which to anneal epsilon\n",
    "    INITIAL_RANDOM_ACTION_PROB = 0.05  # starting chance of an action being random\n",
    "    FINAL_RANDOM_ACTION_PROB = 0.05  # final chance of an action being random\n",
    "    MEMORY_SIZE = 50000  # number of observations to remember\n",
    "    MINI_BATCH_SIZE = 100  # size of mini batches\n",
    "    STATE_FRAMES = 4  # number of frames to store in the state\n",
    "    RESIZED_SCREEN_X, RESIZED_SCREEN_Y = (80, 80)\n",
    "    OBS_LAST_STATE_INDEX, OBS_ACTION_INDEX, OBS_REWARD_INDEX, OBS_CURRENT_STATE_INDEX, OBS_TERMINAL_INDEX = range(5)\n",
    "    SAVE_EVERY_X_STEPS = 49999\n",
    "    LEARN_RATE = 1e-6\n",
    "    STORE_SCORES_LEN = 200.\n",
    "\n",
    "    def __init__(self, checkpoint_path=\"checkpoint\", playback_mode=False, verbose_logging=False):\n",
    "        \"\"\"\n",
    "        Example of deep q network for pong\n",
    "\n",
    "        :param checkpoint_path: directory to store checkpoints in\n",
    "        :type checkpoint_path: str\n",
    "        :param playback_mode: if true games runs in real time mode and demos itself running\n",
    "        :type playback_mode: bool\n",
    "        :param verbose_logging: If true then extra log information is printed to std out\n",
    "        :type verbose_logging: bool\n",
    "        \"\"\"\n",
    "        self.reward_history = deque()\n",
    "        self.reward_memory = 0\n",
    "        \n",
    "        self._playback_mode = playback_mode\n",
    "        super(DeepQPongPlayerTest, self).__init__(force_game_fps=8)\n",
    "        self.verbose_logging = verbose_logging\n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        if 1:\n",
    "            self._session = tf.Session()\n",
    "            self._input_layer, self._output_layer = DeepQPongPlayerTest._create_network()\n",
    "\n",
    "            self._action = tf.placeholder(\"float\", [None, self.ACTIONS_COUNT])\n",
    "            self._target = tf.placeholder(\"float\", [None])\n",
    "\n",
    "            readout_action = tf.reduce_sum(tf.multiply(self._output_layer, self._action), reduction_indices=1)\n",
    "\n",
    "            cost = tf.reduce_mean(tf.square(self._target - readout_action))\n",
    "            #self._train_operation = tf.train.AdamOptimizer(self.LEARN_RATE).minimize(cost)\n",
    "\n",
    "        self._observations = deque()\n",
    "        self._last_scores = deque()\n",
    "\n",
    "        # set the first action to do nothing\n",
    "        self._last_action = np.zeros(self.ACTIONS_COUNT)\n",
    "        self._last_action[1] = 1\n",
    "\n",
    "        self._last_state = None\n",
    "        self._probability_of_random_action = self.INITIAL_RANDOM_ACTION_PROB\n",
    "        self._time = 0\n",
    "\n",
    "        self._session.run(tf.global_variables_initializer())\n",
    "\n",
    "        if not os.path.exists(self._checkpoint_path):\n",
    "            os.mkdir(self._checkpoint_path)\n",
    "        \n",
    "        self._saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(self._checkpoint_path)\n",
    "\n",
    "        if 1:\n",
    "            self._saver.restore(self._session, checkpoint.model_checkpoint_path)\n",
    "            print(\"Loaded checkpoints %s\" % checkpoint.model_checkpoint_path)\n",
    "\n",
    "    def get_keys_pressed(self, screen_array, reward, terminal):\n",
    "        # scale down screen image\n",
    "        screen_resized_grayscaled = cv2.cvtColor(cv2.resize(screen_array,\n",
    "                                                            (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y)),\n",
    "                                                 cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # set the pixels to all be 0. or 1.\n",
    "        _, screen_resized_binary = cv2.threshold(screen_resized_grayscaled, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #if reward != 0.0:\n",
    "            # self._last_scores.append(reward)\n",
    "            #if len(self._last_scores) > self.STORE_SCORES_LEN:\n",
    "            #    self._last_scores.popleft()\n",
    "\n",
    "        # first frame must be handled differently\n",
    "        if self._last_state is None:\n",
    "            # the _last_state will contain the image data from the last self.STATE_FRAMES frames\n",
    "            self._last_state = np.stack(tuple(screen_resized_binary for _ in range(self.STATE_FRAMES)), axis=2)\n",
    "\n",
    "            return DeepQPongPlayerTest._key_presses_from_action(self._last_action)\n",
    "\n",
    "        screen_resized_binary = np.reshape(screen_resized_binary,\n",
    "                                               (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y, 1)) ## 图像处理结束\n",
    "        \n",
    "        current_state = np.append(self._last_state[:, :, 1:], screen_resized_binary, axis=2)\n",
    "\n",
    "\n",
    "        # store the transition in previous_observations\n",
    "        self._observations.append((self._last_state, self._last_action, reward, current_state, terminal))\n",
    "\n",
    "\n",
    "\n",
    "            # only train if done observing\n",
    "\n",
    "        self._time += 1\n",
    "            \n",
    "        # update the old values\n",
    "        self._last_state = current_state\n",
    "        \n",
    "        self._last_action = self._choose_next_action() ## 后面会定义好\n",
    "\n",
    "          \n",
    "        self.reward_memory += reward\n",
    "        if (self._time >= self.EXPLORE_STEPS-1):\n",
    "            return [K_RIGHT]    \n",
    "        return DeepQPongPlayerTest._key_presses_from_action(self._last_action)\n",
    "\n",
    "    def _choose_next_action(self):\n",
    "        new_action = np.zeros([self.ACTIONS_COUNT])\n",
    "\n",
    "        if (random.random() <= self._probability_of_random_action):\n",
    "            # choose an action randomly\n",
    "            action_index = random.randrange(self.ACTIONS_COUNT)\n",
    "        else:\n",
    "            # choose an action given our last state\n",
    "            readout_t = self._session.run(self._output_layer, feed_dict={self._input_layer: [self._last_state]})[0]\n",
    "            if self.verbose_logging:\n",
    "                print(\"Action Q-Values are %s\" % readout_t)\n",
    "            action_index = np.argmax(readout_t)\n",
    "\n",
    "        new_action[action_index] = 1\n",
    "        return new_action # \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_network():\n",
    "        # network weights\n",
    "        if 1:\n",
    "            input_layer = tf.placeholder(\"float\", [None, 80, 80,4])   # Input layer\n",
    "            \n",
    "            w_1 = tf.Variable(tf.truncated_normal([8, 8, 4, 16], stddev=0.01))\n",
    "            b_1 = tf.Variable(tf.constant(0.01, shape=[16]))\n",
    "            layer_conv1 = tf.nn.relu(tf.nn.conv2d(input_layer, w_1, strides=[1, 4, 4, 1], padding=\"SAME\") + b_1,\n",
    "                                name= 'Conv1')\n",
    "            layer_pool1 = tf.nn.max_pool(layer_conv1, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "            #FIRST HIDDEN LAYER: CONV LAYER, SHAPE =(?, 10, 10, 16)\n",
    "            # Note:\n",
    "            # The paper of Deep Mind didn't specify a specific form of recifier nonlinearity. Here we use ReLU, \n",
    "            # which is often useful in computer vision topics. \n",
    "             \n",
    "            w_2 = tf.Variable(tf.truncated_normal([4, 4, 16, 32], stddev=0.01))\n",
    "            b_2 = tf.Variable(tf.constant(0.01, shape=[32]))            \n",
    "            layer_conv2 = tf.nn.relu(tf.nn.conv2d(layer_pool1, w_2, strides=[1, 2, 2, 1], padding=\"SAME\") + b_2,\n",
    "                                name = 'Conv2')            \n",
    "            # SECOND HIDDEN LAYER: CONV LAYER, SHAPE =(?, 5, 5, 32)\n",
    "            \n",
    "            layer_2_flat = tf.reshape(layer_conv2, [-1, 5 *5 * 32])\n",
    "            layer_dense = tf.layers.dense(inputs=layer_2_flat, units=256,  activation=tf.nn.relu) \n",
    "            # THIRD HIDDEN LAYER: FULLY-CONNECTED(DENSE) LAYER, SHAPE = (?, 256)\n",
    "            \n",
    "            dropout = tf.layers.dropout(layer_dense, rate=0.2, name = 'Dropout_layer')\n",
    "            # REGULARIZATION LAYER: TO AVOID OVERFITTING, SHAPE = (?, 256)\n",
    "            \n",
    "            output_layer = tf.layers.dense(inputs=dropout, units=3) \n",
    "            # OUTPUT LAYER\n",
    "\n",
    "\n",
    "        return input_layer, output_layer \n",
    "\n",
    "    @staticmethod\n",
    "    def _key_presses_from_action(action_set):\n",
    "        if action_set[0] == 1:\n",
    "            return [K_DOWN]\n",
    "        elif action_set[1] == 1:\n",
    "            return []\n",
    "        elif action_set[2] == 1:\n",
    "            return [K_UP]\n",
    "        raise Exception(\"Unexpected action\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starts pygame window\n",
    "player = DeepQPongPlayerTest()\n",
    "player.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"600\" height=\"400\" controls><source src=\"Desktop/AML_final_project_1216/aml_model4/Test_model3.mp4\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"Desktop/AML_final_project_1216/aml_model4/Test_model3.mp4\" type=\"video/mp4\"></video>\"\"\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
