{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ecbm4040/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Restoring parameters from deep_q_pong_networks/network-49900\n",
      "Loaded checkpoints deep_q_pong_networks/network-49900\n",
      "Time: 200; Reward: -0.0608333333333\n",
      "Time: 300; Reward: -0.03\n",
      "Time: 400; Reward: -0.06\n",
      "Time: 500; Reward: -0.04\n",
      "Time: 600; Reward: -0.03\n",
      "Time: 700; Reward: -0.09\n",
      "Time: 800; Reward: -0.04\n",
      "Time: 900; Reward: -0.08\n",
      "Time: 1000; Reward: -0.07\n",
      "Time: 1100; Reward: -0.09\n",
      "Time: 1200; Reward: -0.09\n",
      "Time: 1300; Reward: -0.03\n",
      "Time: 1400; Reward: -0.02\n",
      "Time: 1500; Reward: -0.02\n",
      "Time: 1600; Reward: -0.09\n",
      "Time: 1700; Reward: -0.04\n",
      "Time: 1800; Reward: -0.03\n",
      "Time: 1900; Reward: -0.04\n",
      "Time: 2000; Reward: 0.0\n",
      "Time: 2100; Reward: -0.01\n",
      "Time: 2200; Reward: -0.09\n",
      "Time: 2300; Reward: -0.04\n",
      "Time: 2400; Reward: -0.09\n",
      "Time: 2500; Reward: -0.03\n",
      "Time: 2600; Reward: -0.03\n",
      "Time: 2700; Reward: -0.07\n",
      "Time: 2800; Reward: -0.04\n",
      "Time: 2900; Reward: -0.05\n",
      "Time: 3000; Reward: -0.06\n",
      "Time: 3100; Reward: -0.03\n",
      "Time: 3200; Reward: -0.04\n",
      "Time: 3300; Reward: -0.04\n",
      "Time: 3400; Reward: -0.05\n",
      "Time: 3500; Reward: -0.03\n",
      "Time: 3600; Reward: -0.02\n",
      "Time: 3700; Reward: 0.02\n",
      "Time: 3800; Reward: -0.04\n",
      "Time: 3900; Reward: -0.07\n",
      "Time: 4000; Reward: -0.06\n",
      "Time: 4100; Reward: -0.05\n",
      "Time: 4200; Reward: -0.04\n",
      "Time: 4300; Reward: -0.03\n",
      "Time: 4400; Reward: -0.02\n",
      "Time: 4500; Reward: 0.0\n",
      "Time: 4600; Reward: 0.0\n",
      "Time: 4700; Reward: -0.04\n",
      "Time: 4800; Reward: -0.05\n",
      "Time: 4900; Reward: -0.04\n",
      "Time: 5000; Reward: -0.02\n",
      "Time: 5100; Reward: -0.07\n",
      "Time: 5200; Reward: -0.09\n",
      "Time: 5300; Reward: -0.04\n",
      "Time: 5400; Reward: -0.07\n",
      "Time: 5500; Reward: -0.09\n",
      "Time: 5600; Reward: -0.05\n",
      "Time: 5700; Reward: -0.05\n",
      "Time: 5800; Reward: -0.07\n",
      "Time: 5900; Reward: -0.02\n",
      "Time: 6000; Reward: -0.09\n",
      "Time: 6100; Reward: -0.03\n",
      "Time: 6200; Reward: -0.06\n",
      "Time: 6300; Reward: -0.09\n",
      "Time: 6400; Reward: -0.08\n",
      "Time: 6500; Reward: -0.05\n",
      "Time: 6600; Reward: -0.09\n",
      "Time: 6700; Reward: -0.03\n",
      "Time: 6800; Reward: -0.06\n",
      "Time: 6900; Reward: -0.09\n",
      "Time: 7000; Reward: -0.09\n",
      "Time: 7100; Reward: 0.0\n",
      "Time: 7200; Reward: -0.06\n",
      "Time: 7300; Reward: -0.05\n",
      "Time: 7400; Reward: -0.09\n",
      "Time: 7500; Reward: -0.05\n",
      "Time: 7600; Reward: -0.08\n",
      "Time: 7700; Reward: -0.08\n",
      "Time: 7800; Reward: -0.02\n",
      "Time: 7900; Reward: -0.08\n",
      "Time: 8000; Reward: -0.06\n",
      "Time: 8100; Reward: -0.02\n",
      "Time: 8200; Reward: -0.01\n",
      "Time: 8300; Reward: -0.08\n",
      "Time: 8400; Reward: -0.04\n",
      "Time: 8500; Reward: -0.04\n",
      "Time: 8600; Reward: -0.03\n",
      "Time: 8700; Reward: -0.03\n",
      "Time: 8800; Reward: -0.04\n",
      "Time: 8900; Reward: -0.03\n",
      "Time: 9000; Reward: 0.01\n",
      "Time: 9100; Reward: -0.03\n",
      "Time: 9200; Reward: 0.01\n",
      "Time: 9300; Reward: -0.05\n",
      "Time: 9400; Reward: -0.03\n",
      "Time: 9500; Reward: -0.09\n",
      "Time: 9600; Reward: -0.08\n",
      "Time: 9700; Reward: -0.05\n",
      "Time: 9800; Reward: -0.06\n",
      "Time: 9900; Reward: -0.09\n",
      "Time: 10000; Reward: -0.09\n",
      "Time: 10100; Reward: -0.05\n",
      "Time: 10200; Reward: 0.0\n",
      "Time: 10300; Reward: -0.04\n",
      "Time: 10400; Reward: -0.06\n",
      "Time: 10500; Reward: -0.09\n",
      "Time: 10600; Reward: -0.02\n",
      "Time: 10700; Reward: 0.01\n",
      "Time: 10800; Reward: -0.04\n",
      "Time: 10900; Reward: -0.05\n",
      "Time: 11000; Reward: -0.09\n",
      "Time: 11100; Reward: -0.06\n",
      "Time: 11200; Reward: -0.09\n",
      "Time: 11300; Reward: -0.04\n",
      "Time: 11400; Reward: 0.01\n",
      "Time: 11500; Reward: -0.08\n",
      "Time: 11600; Reward: 0.0\n",
      "Time: 11700; Reward: -0.02\n",
      "Time: 11800; Reward: -0.09\n",
      "Time: 11900; Reward: -0.09\n",
      "Time: 12000; Reward: -0.08\n",
      "Time: 12100; Reward: -0.05\n",
      "Time: 12200; Reward: 0.01\n",
      "Time: 12300; Reward: 0.0\n",
      "Time: 12400; Reward: 0.01\n",
      "Time: 12500; Reward: -0.09\n",
      "Time: 12600; Reward: -0.01\n",
      "Time: 12700; Reward: -0.02\n",
      "Time: 12800; Reward: 0.0\n",
      "Time: 12900; Reward: -0.04\n",
      "Time: 13000; Reward: -0.09\n",
      "Time: 13100; Reward: -0.08\n",
      "Time: 13200; Reward: -0.05\n",
      "Time: 13300; Reward: -0.05\n",
      "Time: 13400; Reward: -0.04\n",
      "Time: 13500; Reward: -0.02\n",
      "Time: 13600; Reward: -0.01\n",
      "Time: 13700; Reward: -0.07\n",
      "Time: 13800; Reward: -0.01\n",
      "Time: 13900; Reward: -0.02\n",
      "Time: 14000; Reward: -0.05\n",
      "Time: 14100; Reward: 0.0\n",
      "Time: 14200; Reward: -0.06\n",
      "Time: 14300; Reward: -0.06\n",
      "Time: 14400; Reward: -0.01\n",
      "Time: 14500; Reward: -0.04\n",
      "Time: 14600; Reward: -0.03\n",
      "Time: 14700; Reward: -0.09\n",
      "Time: 14800; Reward: -0.06\n",
      "Time: 14900; Reward: 0.01\n"
     ]
    }
   ],
   "source": [
    "# This is heavily based off https://github.com/asrivat1/DeepLearningVideoGames\n",
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import random\n",
    "from collections import deque\n",
    "from pong_player import PongPlayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pygame.constants import K_DOWN, K_UP, K_RIGHT\n",
    "\n",
    "reward_history = []\n",
    "reward_memory = []\n",
    "class DeepQPongPlayer(PongPlayer):\n",
    "    ACTIONS_COUNT = 3  # number of valid actions. In this case up, still and down\n",
    "    FUTURE_REWARD_DISCOUNT = 0.99  # decay rate of past observations\n",
    "    OBSERVATION_STEPS = 1000.  # time steps to observe before training\n",
    "    EXPLORE_STEPS = 50000.  # frames over which to anneal epsilon\n",
    "    INITIAL_RANDOM_ACTION_PROB = 1.0  # starting chance of an action being random\n",
    "    FINAL_RANDOM_ACTION_PROB = 0.05  # final chance of an action being random\n",
    "    MEMORY_SIZE = 50000  # number of observations to remember\n",
    "    MINI_BATCH_SIZE = 100  # size of mini batches\n",
    "    STATE_FRAMES = 4  # number of frames to store in the state\n",
    "    RESIZED_SCREEN_X, RESIZED_SCREEN_Y = (80, 80)\n",
    "    OBS_LAST_STATE_INDEX, OBS_ACTION_INDEX, OBS_REWARD_INDEX, OBS_CURRENT_STATE_INDEX, OBS_TERMINAL_INDEX = range(5)\n",
    "    SAVE_EVERY_X_STEPS = 100\n",
    "    LEARN_RATE = 1e-6\n",
    "    STORE_SCORES_LEN = 200.\n",
    "    reward_history = [] ## delete later\n",
    "    reward_memory = [] ## too\n",
    "    def __init__(self, checkpoint_path=\"deep_q_pong_networks\", playback_mode=False, verbose_logging=False):\n",
    "        \"\"\"\n",
    "        Example of deep q network for pong\n",
    "\n",
    "        :param checkpoint_path: directory to store checkpoints in\n",
    "        :type checkpoint_path: str\n",
    "        :param playback_mode: if true games runs in real time mode and demos itself running\n",
    "        :type playback_mode: bool\n",
    "        :param verbose_logging: If true then extra log information is printed to std out\n",
    "        :type verbose_logging: bool\n",
    "        \"\"\"\n",
    "        self.reward_history = []\n",
    "        self.reward_memory = []\n",
    "        \n",
    "        self._playback_mode = playback_mode\n",
    "        super(DeepQPongPlayer, self).__init__(force_game_fps=8, run_real_time=playback_mode)\n",
    "        self.verbose_logging = verbose_logging\n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        with tf.device('/gpu:0'):\n",
    "            self._session = tf.Session()\n",
    "            self._input_layer, self._output_layer = DeepQPongPlayer._create_network()\n",
    "\n",
    "            self._action = tf.placeholder(\"float\", [None, self.ACTIONS_COUNT])\n",
    "            self._target = tf.placeholder(\"float\", [None])\n",
    "\n",
    "            readout_action = tf.reduce_sum(tf.multiply(self._output_layer, self._action), reduction_indices=1)\n",
    "\n",
    "            cost = tf.reduce_mean(tf.square(self._target - readout_action))\n",
    "            self._train_operation = tf.train.AdamOptimizer(self.LEARN_RATE).minimize(cost)\n",
    "\n",
    "        self._observations = deque()\n",
    "        self._last_scores = deque()\n",
    "\n",
    "        # set the first action to do nothing\n",
    "        self._last_action = np.zeros(self.ACTIONS_COUNT)\n",
    "        self._last_action[1] = 1\n",
    "\n",
    "        self._last_state = None\n",
    "        self._probability_of_random_action = self.INITIAL_RANDOM_ACTION_PROB\n",
    "        self._time = 0\n",
    "\n",
    "        self._session.run(tf.initialize_all_variables())\n",
    "\n",
    "        if not os.path.exists(self._checkpoint_path):\n",
    "            os.mkdir(self._checkpoint_path)\n",
    "        \n",
    "        self._saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(self._checkpoint_path)\n",
    "\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self._saver.restore(self._session, checkpoint.model_checkpoint_path)\n",
    "            print(\"Loaded checkpoints %s\" % checkpoint.model_checkpoint_path)\n",
    "        elif playback_mode:\n",
    "            raise Exception(\"Could not load checkpoints for playback\")\n",
    "\n",
    "    def get_keys_pressed(self, screen_array, reward, terminal):\n",
    "        # scale down screen image\n",
    "        screen_resized_grayscaled = cv2.cvtColor(cv2.resize(screen_array,\n",
    "                                                            (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y)),\n",
    "                                                 cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # set the pixels to all be 0. or 1.\n",
    "        _, screen_resized_binary = cv2.threshold(screen_resized_grayscaled, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        if reward != 0.0:\n",
    "            self._last_scores.append(reward)\n",
    "            if len(self._last_scores) > self.STORE_SCORES_LEN:\n",
    "                self._last_scores.popleft()\n",
    "\n",
    "        # first frame must be handled differently\n",
    "        if self._last_state is None:\n",
    "            # the _last_state will contain the image data from the last self.STATE_FRAMES frames\n",
    "            self._last_state = np.stack(tuple(screen_resized_binary for _ in range(self.STATE_FRAMES)), axis=2)\n",
    "\n",
    "            return DeepQPongPlayer._key_presses_from_action(self._last_action)\n",
    "\n",
    "        screen_resized_binary = np.reshape(screen_resized_binary,\n",
    "                                               (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y, 1)) ## 图像处理结束\n",
    "        \n",
    "        current_state = np.append(self._last_state[:, :, 1:], screen_resized_binary, axis=2)\n",
    "\n",
    "        if not self._playback_mode:\n",
    "            # store the transition in previous_observations\n",
    "            self._observations.append((self._last_state, self._last_action, reward, current_state, terminal))\n",
    "\n",
    "            if len(self._observations) > self.MEMORY_SIZE:\n",
    "                self._observations.popleft()\n",
    "\n",
    "            # only train if done observing\n",
    "            if len(self._observations) > self.OBSERVATION_STEPS:\n",
    "                self._train()\n",
    "                self._time += 1\n",
    "\n",
    "        # update the old values\n",
    "        self._last_state = current_state\n",
    "\n",
    "        self._last_action = self._choose_next_action() ## 后面会定义好\n",
    "\n",
    "        if not self._playback_mode:\n",
    "            # gradually reduce the probability of a random actionself.\n",
    "            if self._probability_of_random_action > self.FINAL_RANDOM_ACTION_PROB \\\n",
    "                    and len(self._observations) > self.OBSERVATION_STEPS:\n",
    "                self._probability_of_random_action -= \\\n",
    "                    (self.INITIAL_RANDOM_ACTION_PROB - self.FINAL_RANDOM_ACTION_PROB) / self.EXPLORE_STEPS\n",
    "            \"\"\"\n",
    "            print(\"Time: %s random_action_prob: %s reward %s scores differential %s\" %\n",
    "                  (self._time, self._probability_of_random_action, reward,\n",
    "                   sum(self._last_scores) / self.STORE_SCORES_LEN))\n",
    "            \"\"\"\n",
    "            self.reward_memory.append(reward)\n",
    "            if (self._time > 100 and self._time % 100 == 0):\n",
    "                self.reward_history.append(np.mean(self.reward_memory))\n",
    "                self.reward_memory = []\n",
    "                print(\"Time: %s; Reward: %s\" % (self._time, self.reward_history[-1]))\n",
    "            if (self._time >= self.EXPLORE_STEPS-1):\n",
    "                #print(\"我退出了 = =\")\n",
    "                return [K_RIGHT]    \n",
    "        return DeepQPongPlayer._key_presses_from_action(self._last_action)\n",
    "\n",
    "    def _choose_next_action(self):\n",
    "        new_action = np.zeros([self.ACTIONS_COUNT])\n",
    "\n",
    "        if (not self._playback_mode) and (random.random() <= self._probability_of_random_action):\n",
    "            # choose an action randomly\n",
    "            action_index = random.randrange(self.ACTIONS_COUNT)\n",
    "        else:\n",
    "            # choose an action given our last state\n",
    "            readout_t = self._session.run(self._output_layer, feed_dict={self._input_layer: [self._last_state]})[0]\n",
    "            if self.verbose_logging:\n",
    "                print(\"Action Q-Values are %s\" % readout_t)\n",
    "            action_index = np.argmax(readout_t)\n",
    "\n",
    "        new_action[action_index] = 1\n",
    "        return new_action # \n",
    "\n",
    "    def _train(self):\n",
    "        # sample a mini_batch to train on\n",
    "        mini_batch = random.sample(self._observations, self.MINI_BATCH_SIZE)\n",
    "        # get the batch variables\n",
    "        previous_states = [d[self.OBS_LAST_STATE_INDEX] for d in mini_batch]\n",
    "        actions = [d[self.OBS_ACTION_INDEX] for d in mini_batch]\n",
    "        rewards = [d[self.OBS_REWARD_INDEX] for d in mini_batch]\n",
    "        current_states = [d[self.OBS_CURRENT_STATE_INDEX] for d in mini_batch]\n",
    "        agents_expected_reward = []\n",
    "        # this gives us the agents expected reward for each action we might\n",
    "        agents_reward_per_action = self._session.run(self._output_layer, feed_dict={self._input_layer: current_states})\n",
    "        for i in range(len(mini_batch)):\n",
    "            if mini_batch[i][self.OBS_TERMINAL_INDEX]:\n",
    "                # this was a terminal frame so there is no future reward...\n",
    "                agents_expected_reward.append(rewards[i])\n",
    "            else:\n",
    "                agents_expected_reward.append(\n",
    "                    rewards[i] + self.FUTURE_REWARD_DISCOUNT * np.max(agents_reward_per_action[i]))\n",
    "\n",
    "        # learn that these actions in these states lead to this reward\n",
    "        self._session.run(self._train_operation, feed_dict={\n",
    "            self._input_layer: previous_states,\n",
    "            self._action: actions,\n",
    "            self._target: agents_expected_reward})\n",
    "\n",
    "        # save checkpoints for later\n",
    "        if self._time % self.SAVE_EVERY_X_STEPS == 0:\n",
    "            self._saver.save(self._session, self._checkpoint_path + '/network', global_step=self._time)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_network():\n",
    "        # network weights\n",
    "        with tf.device('/gpu:0'):\n",
    "            convolution_weights_1 = tf.Variable(tf.truncated_normal([8, 8, DeepQPongPlayer.STATE_FRAMES, 32], stddev=0.01))\n",
    "            convolution_bias_1 = tf.Variable(tf.constant(0.01, shape=[32]))\n",
    "\n",
    "            convolution_weights_2 = tf.Variable(tf.truncated_normal([4, 4, 32, 64], stddev=0.01))\n",
    "            convolution_bias_2 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "\n",
    "            convolution_weights_3 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev=0.01))\n",
    "            convolution_bias_3 = tf.Variable(tf.constant(0.01, shape=[64]))\n",
    "\n",
    "            feed_forward_weights_1 = tf.Variable(tf.truncated_normal([256, 256], stddev=0.01))\n",
    "            feed_forward_bias_1 = tf.Variable(tf.constant(0.01, shape=[256]))\n",
    "\n",
    "            feed_forward_weights_2 = tf.Variable(tf.truncated_normal([256, DeepQPongPlayer.ACTIONS_COUNT], stddev=0.01))\n",
    "            feed_forward_bias_2 = tf.Variable(tf.constant(0.01, shape=[DeepQPongPlayer.ACTIONS_COUNT]))\n",
    "\n",
    "            input_layer = tf.placeholder(\"float\", [None, DeepQPongPlayer.RESIZED_SCREEN_X, DeepQPongPlayer.RESIZED_SCREEN_Y,\n",
    "                                               DeepQPongPlayer.STATE_FRAMES])\n",
    "\n",
    "            hidden_convolutional_layer_1 = tf.nn.relu(\n",
    "                tf.nn.conv2d(input_layer, convolution_weights_1, strides=[1, 4, 4, 1], padding=\"SAME\") + convolution_bias_1)\n",
    "\n",
    "            hidden_max_pooling_layer_1 = tf.nn.max_pool(hidden_convolutional_layer_1, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "            hidden_convolutional_layer_2 = tf.nn.relu(\n",
    "                tf.nn.conv2d(hidden_max_pooling_layer_1, convolution_weights_2, strides=[1, 2, 2, 1],\n",
    "                         padding=\"SAME\") + convolution_bias_2)\n",
    "\n",
    "            hidden_max_pooling_layer_2 = tf.nn.max_pool(hidden_convolutional_layer_2, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "            hidden_convolutional_layer_3 = tf.nn.relu(\n",
    "                tf.nn.conv2d(hidden_max_pooling_layer_2, convolution_weights_3,\n",
    "                         strides=[1, 1, 1, 1], padding=\"SAME\") + convolution_bias_3)\n",
    "\n",
    "            hidden_max_pooling_layer_3 = tf.nn.max_pool(hidden_convolutional_layer_3, ksize=[1, 2, 2, 1],\n",
    "                                                    strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "            hidden_convolutional_layer_3_flat = tf.reshape(hidden_max_pooling_layer_3, [-1, 256])\n",
    "\n",
    "            final_hidden_activations = tf.nn.relu(\n",
    "                tf.matmul(hidden_convolutional_layer_3_flat, feed_forward_weights_1) + feed_forward_bias_1)\n",
    "\n",
    "            output_layer = tf.matmul(final_hidden_activations, feed_forward_weights_2) + feed_forward_bias_2\n",
    "\n",
    "        return input_layer, output_layer \n",
    "\n",
    "    @staticmethod\n",
    "    def _key_presses_from_action(action_set):\n",
    "        if action_set[0] == 1:\n",
    "            return [K_DOWN]\n",
    "        elif action_set[1] == 1:\n",
    "            return []\n",
    "        elif action_set[2] == 1:\n",
    "            return [K_UP]\n",
    "        raise Exception(\"Unexpected action\") \n",
    "\n",
    "\n",
    "\n",
    "player = DeepQPongPlayer()\n",
    "player.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
