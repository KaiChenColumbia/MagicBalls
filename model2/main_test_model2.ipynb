{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model2_checkpoint/network-599988\n",
      "Loaded checkpoints model2_checkpoint/network-599988\n"
     ]
    }
   ],
   "source": [
    "# This is heavily based off https://github.com/asrivat1/DeepLearningVideoGames\n",
    "import os\n",
    "# os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "import pygame\n",
    "import random\n",
    "from collections import deque\n",
    "from pong_player import PongPlayer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pygame.constants import K_DOWN, K_UP, K_RIGHT\n",
    "\n",
    "\n",
    "class DeepQPongPlayerTest(PongPlayer):\n",
    "    ACTIONS_COUNT = 3  # number of valid actions. In this case up, still and down\n",
    "    # FUTURE_REWARD_DISCOUNT = 0.99  # decay rate of past observations\n",
    "    \n",
    "    EXPLORE_STEPS = 500000.  # frames over which to anneal epsilon\n",
    "    INITIAL_RANDOM_ACTION_PROB = 0.05  # starting chance of an action being random\n",
    "    FINAL_RANDOM_ACTION_PROB = 0.05  # final chance of an action being random\n",
    "    MEMORY_SIZE = 50000  # number of observations to remember\n",
    "    MINI_BATCH_SIZE = 100  # size of mini batches\n",
    "    STATE_FRAMES = 4  # number of frames to store in the state\n",
    "    RESIZED_SCREEN_X, RESIZED_SCREEN_Y = (80, 80)\n",
    "    OBS_LAST_STATE_INDEX, OBS_ACTION_INDEX, OBS_REWARD_INDEX, OBS_CURRENT_STATE_INDEX, OBS_TERMINAL_INDEX = range(5)\n",
    "    SAVE_EVERY_X_STEPS = 49999\n",
    "    LEARN_RATE = 1e-6\n",
    "    STORE_SCORES_LEN = 200.\n",
    "\n",
    "    def __init__(self, checkpoint_path=\"deep_q_pong_networks\", playback_mode=False, verbose_logging=False):\n",
    "        \"\"\"\n",
    "        Example of deep q network for pong\n",
    "\n",
    "        :param checkpoint_path: directory to store checkpoints in\n",
    "        :type checkpoint_path: str\n",
    "        :param playback_mode: if true games runs in real time mode and demos itself running\n",
    "        :type playback_mode: bool\n",
    "        :param verbose_logging: If true then extra log information is printed to std out\n",
    "        :type verbose_logging: bool\n",
    "        \"\"\"\n",
    "        self.reward_history = deque()\n",
    "        self.reward_memory = 0\n",
    "        \n",
    "        self._playback_mode = playback_mode\n",
    "        super(DeepQPongPlayerTest, self).__init__(force_game_fps=8)\n",
    "        self.verbose_logging = verbose_logging\n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        if 1:\n",
    "            self._session = tf.Session()\n",
    "            self._input_layer, self._output_layer = DeepQPongPlayerTest._create_network()\n",
    "\n",
    "            self._action = tf.placeholder(\"float\", [None, self.ACTIONS_COUNT])\n",
    "            self._target = tf.placeholder(\"float\", [None])\n",
    "\n",
    "            readout_action = tf.reduce_sum(tf.multiply(self._output_layer, self._action), reduction_indices=1)\n",
    "\n",
    "            cost = tf.reduce_mean(tf.square(self._target - readout_action))\n",
    "            #self._train_operation = tf.train.AdamOptimizer(self.LEARN_RATE).minimize(cost)\n",
    "\n",
    "        self._observations = deque()\n",
    "        self._last_scores = deque()\n",
    "\n",
    "        # set the first action to do nothing\n",
    "        self._last_action = np.zeros(self.ACTIONS_COUNT)\n",
    "        self._last_action[1] = 1\n",
    "\n",
    "        self._last_state = None\n",
    "        self._probability_of_random_action = self.INITIAL_RANDOM_ACTION_PROB\n",
    "        self._time = 0\n",
    "\n",
    "        self._session.run(tf.global_variables_initializer())\n",
    "\n",
    "        if not os.path.exists(self._checkpoint_path):\n",
    "            os.mkdir(self._checkpoint_path)\n",
    "        \n",
    "        self._saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(self._checkpoint_path)\n",
    "\n",
    "        if (checkpoint):\n",
    "            self._saver.restore(self._session, checkpoint.model_checkpoint_path)\n",
    "            print(\"Loaded checkpoints %s\" % checkpoint.model_checkpoint_path)\n",
    "\n",
    "    def get_keys_pressed(self, screen_array, reward, terminal):\n",
    "        # scale down screen image\n",
    "        screen_resized_grayscaled = cv2.cvtColor(cv2.resize(screen_array,\n",
    "                                                            (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y)),\n",
    "                                                 cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # set the pixels to all be 0. or 1.\n",
    "        _, screen_resized_binary = cv2.threshold(screen_resized_grayscaled, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #if reward != 0.0:\n",
    "            # self._last_scores.append(reward)\n",
    "            #if len(self._last_scores) > self.STORE_SCORES_LEN:\n",
    "            #    self._last_scores.popleft()\n",
    "\n",
    "        # first frame must be handled differently\n",
    "        if self._last_state is None:\n",
    "            # the _last_state will contain the image data from the last self.STATE_FRAMES frames\n",
    "            self._last_state = np.stack(tuple(screen_resized_binary for _ in range(self.STATE_FRAMES)), axis=2)\n",
    "\n",
    "            return DeepQPongPlayerTest._key_presses_from_action(self._last_action)\n",
    "\n",
    "        screen_resized_binary = np.reshape(screen_resized_binary,\n",
    "                                               (self.RESIZED_SCREEN_X, self.RESIZED_SCREEN_Y, 1)) \n",
    "        \n",
    "        current_state = np.append(self._last_state[:, :, 1:], screen_resized_binary, axis=2)\n",
    "\n",
    "\n",
    "        # store the transition in previous_observations\n",
    "        self._observations.append((self._last_state, self._last_action, reward, current_state, terminal))\n",
    "\n",
    "\n",
    "\n",
    "            # only train if done observing\n",
    "\n",
    "        self._time += 1\n",
    "            \n",
    "        # update the old values\n",
    "        self._last_state = current_state\n",
    "        \n",
    "        self._last_action = self._choose_next_action()\n",
    "\n",
    "          \n",
    "        self.reward_memory += reward\n",
    "        if (self._time >= self.EXPLORE_STEPS-1):\n",
    "            return [K_RIGHT]    \n",
    "        return DeepQPongPlayerTest._key_presses_from_action(self._last_action)\n",
    "\n",
    "    def _choose_next_action(self):\n",
    "        new_action = np.zeros([self.ACTIONS_COUNT])\n",
    "\n",
    "        if (random.random() <= self._probability_of_random_action):\n",
    "            # choose an action randomly\n",
    "            action_index = random.randrange(self.ACTIONS_COUNT)\n",
    "        else:\n",
    "            # choose an action given our last state\n",
    "            readout_t = self._session.run(self._output_layer, feed_dict={self._input_layer: [self._last_state]})[0]\n",
    "            if self.verbose_logging:\n",
    "                print(\"Action Q-Values are %s\" % readout_t)\n",
    "            action_index = np.argmax(readout_t)\n",
    "\n",
    "        new_action[action_index] = 1\n",
    "        return new_action # \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_network():\n",
    "        # network weights\n",
    "        if 1:\n",
    "            input_layer = tf.placeholder(\"float\", [None, 80, 80,4])   # Input layer\n",
    "            \n",
    "            \n",
    "            w_1 = tf.Variable(tf.truncated_normal([8, 8, 4, 16], stddev=0.01))\n",
    "            b_1 = tf.Variable(tf.constant(0.01, shape=[16]))\n",
    "            layer_conv1 = tf.nn.relu(tf.nn.conv2d(input_layer, w_1, strides=[1, 4, 4, 1], padding=\"SAME\") + b_1,\n",
    "                                name= 'Conv1')\n",
    "            #FIRST HIDDEN LAYER: CONV LAYER, SHAPE =(?, 20, 20, 16)\n",
    "            # Note:\n",
    "            # The paper of Deep Mind didn't specify a specific form of recifier nonlinearity. Here we use ReLU, \n",
    "            # which is often useful in computer vision topics. \n",
    "             \n",
    "            w_2 = tf.Variable(tf.truncated_normal([4, 4, 16, 32], stddev=0.01))\n",
    "            b_2 = tf.Variable(tf.constant(0.01, shape=[32]))            \n",
    "            layer_conv2 = tf.nn.relu(tf.nn.conv2d(layer_conv1, w_2, strides=[1, 2, 2, 1], padding=\"SAME\") + b_2,\n",
    "                                name = 'Conv2')            \n",
    "            # SECOND HIDDEN LAYER: CONV LAYER, SHAPE =(?, 10, 10, 32)\n",
    "            \n",
    "            layer_2_flat = tf.reshape(layer_conv2, [-1, 10 * 10 * 32])\n",
    "            layer_dense = tf.layers.dense(inputs=layer_2_flat, units=256,  activation=tf.nn.relu) \n",
    "            # THIRD HIDDEN LAYER: FULLY-CONNECTED(DENSE) LAYER, SHAPE = (?, 256)\n",
    "            \n",
    "            dropout = tf.layers.dropout(layer_dense, rate=0.2, name = 'Dropout_layer')\n",
    "            # REGULARIZATION LAYER: TO AVOID OVERFITTING, SHAPE = (?, 256)\n",
    "            \n",
    "            output_layer = tf.layers.dense(inputs=dropout, units=3) \n",
    "\n",
    "        return input_layer, output_layer \n",
    "\n",
    "    @staticmethod\n",
    "    def _key_presses_from_action(action_set):\n",
    "        if action_set[0] == 1:\n",
    "            return [K_DOWN]\n",
    "        elif action_set[1] == 1:\n",
    "            return []\n",
    "        elif action_set[2] == 1:\n",
    "            return [K_UP]\n",
    "        raise Exception(\"Unexpected action\") \n",
    "\n",
    "\n",
    "\n",
    "player = DeepQPongPlayerTest(checkpoint_path = \"model2_checkpoint\")\n",
    "player.start()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
